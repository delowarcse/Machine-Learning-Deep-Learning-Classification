{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program is the implementation of  Machine Learning and Deep Learning Techniques such as: Logistic Regression, Decision Tree, Random Forest, SVM, and DNN methods with MinMax Normalization ([0,1]-Normalizaton)\n",
    "\n",
    "# All right reserved by Delowar Hossain, E-mail: delowar_cse_ru@yahoo.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import model_evaluation_utils as meu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load and merge datasets # white = control; red = stroke; wine = data\n",
    "stroke_data = pd.read_csv('Stroke Data.csv', delim_whitespace=False)\n",
    "control_data = pd.read_csv('Healthy Control Data.csv', delim_whitespace=False)\n",
    "\n",
    "# store wine type as an attribute\n",
    "stroke_data['data_type'] = 'stroke'   \n",
    "control_data['data_type'] = 'control'\n",
    "\n",
    "# merge control and stroke data\n",
    "datas = pd.concat([stroke_data, control_data])\n",
    "datas = datas.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# understand dataset features and values\n",
    "datas.head()\n",
    "#stroke_data.head()\n",
    "#control_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Training and Testing Datasets\n",
    "stp_features = datas.iloc[:,:-1]\n",
    "stp_feature_names = stp_features.columns\n",
    "stp_class_labels = np.array(datas['data_type'])\n",
    "\n",
    "stp_train_X, stp_test_X, stp_train_y, stp_test_y = train_test_split(stp_features, stp_class_labels, \n",
    "                                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "print(Counter(stp_train_y), Counter(stp_test_y))\n",
    "print('Features:', list(stp_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# Define the scaler \n",
    "#stp_ss = StandardScaler().fit(stp_train_X)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "stp_ss = MinMaxScaler().fit(stp_train_X)\n",
    "\n",
    "# Scale the train set\n",
    "stp_train_SX = stp_ss.transform(stp_train_X)\n",
    "\n",
    "# Scale the test set\n",
    "stp_test_SX = stp_ss.transform(stp_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Response class labels\n",
    "le = LabelEncoder()\n",
    "le.fit(stp_train_y)\n",
    "# encode wine type labels\n",
    "stp_train_ey = le.transform(stp_train_y)\n",
    "stp_test_ey = le.transform(stp_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stp_lr = LogisticRegression()\n",
    "stp_lr.fit(stp_train_SX, stp_train_y)\n",
    "\n",
    "y_pred_lr = stp_lr.predict(stp_test_SX)\n",
    "y_pred_prob_lr = stp_lr.predict_proba(stp_test_SX)[:,1]\n",
    "\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(stp_test_ey, y_pred_prob_lr)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "auc_lr = auc(fpr_lr, tpr_lr)\n",
    "#print(auc_lr)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.2f})'.format(auc_lr))\n",
    "#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf)) # wtp_lr, wtp_test_SX, wtp_test_y\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Linear Regression ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('LR ROC Curve.png', bbox_inches='tight')\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(stp_test_ey, y_pred_prob_lr)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(wtp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "plt.figure(2)\n",
    "pyplot.plot(lr_recall, lr_precision, marker='.', label='LR')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "fig = plt.figure(2)\n",
    "pyplot.title('Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#fig.savefig('LR Precision-Recall Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate Model Performance\n",
    "stp_lr_predictions = stp_lr.predict(stp_test_SX)\n",
    "#print(wtp_lr_predictions)\n",
    "meu.display_model_performance_metrics(true_labels=stp_test_y, predicted_labels=stp_lr_predictions, \n",
    "                                      classes=['stroke', 'control'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View model ROC curve\n",
    "meu.plot_model_roc_curve(stp_lr, stp_test_SX, stp_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model using Deep Learning (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Response class labels\n",
    "le = LabelEncoder()\n",
    "le.fit(stp_train_y)\n",
    "# encode wine type labels\n",
    "stp_train_ey = le.transform(stp_train_y)\n",
    "stp_test_ey = le.transform(stp_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build & Compile DNN Model Architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "stp_dnn_model = Sequential()\n",
    "stp_dnn_model.add(Dense(16, activation='relu', input_shape=(12,)))\n",
    "stp_dnn_model.add(Dense(16, activation='relu'))\n",
    "stp_dnn_model.add(Dense(16, activation='relu'))\n",
    "stp_dnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "stp_dnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "history = stp_dnn_model.fit(stp_train_SX, stp_train_ey, epochs=100, batch_size=5, \n",
    "                            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on Test dataset\n",
    "stp_dnn_ypred = stp_dnn_model.predict_classes(stp_test_SX)\n",
    "stp_dnn_ypred_prob = stp_dnn_model.predict_proba(stp_test_SX)\n",
    "stp_dnn_predictions = le.inverse_transform(stp_dnn_ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "t = f.suptitle('Deep Neural Net Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epochs = list(range(1,101))\n",
    "fig = plt.figure(1)\n",
    "ax1.plot(epochs, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks([1,50,100])#ax1.set_xticks(epochs)\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "#fig.savefig('DNN Accuracy Curve.png', bbox_inches='tight')\n",
    "\n",
    "fig = plt.figure(2)\n",
    "ax2.plot(epochs, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks([1,25,60])#ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")\n",
    "#fig.savefig('DNN Loss Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=stp_test_y, predicted_labels=stp_dnn_predictions, \n",
    "                                      classes=['stroke', 'control'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "from sklearn.metrics import roc_curve\n",
    "#y_pred_dnn = wtp_dnn_model.predict(wtp_test_X).ravel()\n",
    "fpr_dnn, tpr_dnn, thresholds_dnn = roc_curve(stp_test_ey, stp_dnn_ypred_prob)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "from sklearn.metrics import auc\n",
    "auc_dnn = auc(fpr_dnn, tpr_dnn)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_dnn, tpr_dnn, label='DNN (area = {:.2f})'.format(auc_dnn))\n",
    "#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf)) # wtp_lr, wtp_test_SX, wtp_test_y\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('DNN ROC Curve.png', bbox_inches='tight')\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "dnn_precision, dnn_recall, _ = precision_recall_curve(stp_test_ey, stp_dnn_ypred_prob)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(wtp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "fig = plt.figure(2)\n",
    "pyplot.plot(dnn_recall, dnn_precision, marker='.', label='DNN')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "pyplot.title('Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#fig.savefig('DNN Precision-Recall Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretation Logistic Regression\n",
    "# View Feature importances\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "\n",
    "stp_interpreter = Interpretation(stp_test_SX, feature_names=stp_features.columns)\n",
    "stp_im_model = InMemoryModel(stp_lr.predict_proba, examples=stp_train_SX, target_names=stp_lr.classes_)\n",
    "plots = stp_interpreter.feature_importance.plot_feature_importance(stp_im_model, ascending=False)\n",
    "plt.xlabel('Relative Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for Logistic Regression')\n",
    "fig = plt.figure(1)\n",
    "#fig.savefig('LR Feature Importance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Decision Surface for Logistic Regression\n",
    "feature_indices = [i for i, feature in enumerate(stp_feature_names) \n",
    "                       if feature in ['Abs error XY', 'Variability Y']]\n",
    "\n",
    "meu.plot_model_decision_surface(clf=stp_lr, train_features=stp_train_SX[:, feature_indices], \n",
    "                                train_labels=stp_train_y, plot_step=0.01, cmap=plt.cm.Wistia_r,\n",
    "                                markers=[',', 'o'], alphas=[0.9, 0.6], colors=['r', 'b'])\n",
    "#fig = plt.figure(1)\n",
    "#fig.savefig('Visualize Model Decision Surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Data Quality\n",
    "\n",
    "# Prepare Training and Testing Datasets\n",
    "sqp_features = datas.iloc[:,:-1]\n",
    "sqp_class_labels = np.array(datas['Abs error XY'])\n",
    "sqp_label_names = ['low', 'medium', 'high']\n",
    "sqp_feature_names = stp_features.columns\n",
    "sqp_class_labels = np.array(datas['data_type'])\n",
    "\n",
    "sqp_train_X, sqp_test_X, sqp_train_y, sqp_test_y = train_test_split(sqp_features, sqp_class_labels, \n",
    "                                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "print(Counter(sqp_train_y), Counter(sqp_test_y))\n",
    "print('Features:', list(stp_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# Define the scaler \n",
    "#sqp_ss = StandardScaler().fit(sqp_train_X)\n",
    "sqp_ss = MinMaxScaler().fit(sqp_train_X)\n",
    "\n",
    "# Scale the train set\n",
    "sqp_train_SX = sqp_ss.transform(sqp_train_X)\n",
    "\n",
    "# Scale the test set\n",
    "sqp_test_SX = sqp_ss.transform(sqp_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Model using Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# Train, Predict & Evaluate Model using Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "stp_dt = DecisionTreeClassifier(max_depth=4) # max_depth=4 gives us best result\n",
    "stp_dt.fit(sqp_train_SX, sqp_train_y)\n",
    "\n",
    "y_pred_dt = stp_dt.predict(stp_test_SX)\n",
    "y_pred_prob_dt = stp_dt.predict_proba(stp_test_SX)[:,1]\n",
    "\n",
    "fpr_dt, tpr_dt, thresholds_dt = roc_curve(stp_test_ey, y_pred_prob_dt)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "from sklearn.metrics import auc\n",
    "auc_dt = auc(fpr_dt, tpr_dt)\n",
    "#print(auc_dt)\n",
    "\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_dt, tpr_dt, label='DT (area = {:.2f})'.format(auc_dt))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('DT ROC Curve.png', bbox_inches='tight')\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "dt_precision, dt_recall, _ = precision_recall_curve(stp_test_ey, y_pred_prob_dt)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(wtp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "fig = plt.figure(2)\n",
    "pyplot.plot(dt_recall, dt_precision, marker='.', label='DT')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "pyplot.title('Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#fig.savefig('DT Precision-Recall Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train, Predict & Evaluate Model using Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "sqp_dt = DecisionTreeClassifier(max_depth=4)\n",
    "sqp_dt.fit(sqp_train_SX, sqp_train_y)\n",
    "\n",
    "sqp_dt_predictions = sqp_dt.predict(sqp_test_SX)\n",
    "\n",
    "meu.display_model_performance_metrics(true_labels=sqp_test_y, predicted_labels=sqp_dt_predictions, \n",
    "                                      classes=['stroke', 'control'])#classes=stp_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Model ROC Curve\n",
    "meu.plot_model_roc_curve(sqp_dt, sqp_test_SX, sqp_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Feature Importances from Decision Tree Model\n",
    "sqp_dt_feature_importances = sqp_dt.feature_importances_\n",
    "sqp_dt_feature_names, sqp_dt_feature_scores = zip(*sorted(zip(sqp_feature_names, sqp_dt_feature_importances), \n",
    "                                                          key=lambda x: x[1]))\n",
    "y_position = list(range(len(sqp_dt_feature_names)))\n",
    "fig = plt.figure(1)\n",
    "plt.barh(y_position, sqp_dt_feature_scores, height=0.6, align='center')\n",
    "plt.yticks(y_position , sqp_dt_feature_names)\n",
    "plt.xlabel('Relative Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "t = plt.title('Feature Importances for Decision Tree')\n",
    "#fig.savefig('DT Feature Importance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "\n",
    "from graphviz import Source\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "\n",
    "graph = Source(tree.export_graphviz(sqp_dt, out_file=None, class_names=stp_class_labels,\n",
    "                                    filled=True, rounded=True, special_characters=False,\n",
    "                                    feature_names=sqp_feature_names, max_depth=4))\n",
    "png_data = graph.pipe(format='png')\n",
    "with open('dtree_structure.png','wb') as f:\n",
    "    f.write(png_data)\n",
    "\n",
    "Image(png_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Model using Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# Train, Predict & Evaluate Model using Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# train the model\n",
    "stp_rf = RandomForestClassifier()\n",
    "stp_rf.fit(sqp_train_SX, sqp_train_y)\n",
    "\n",
    "y_pred_rf = stp_rf.predict(stp_test_SX)\n",
    "y_pred_prob_rf = stp_rf.predict_proba(stp_test_SX)[:,1]\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(stp_test_ey, y_pred_prob_rf)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "from sklearn.metrics import auc\n",
    "auc_rf = auc(fpr_rf, tpr_rf)\n",
    "#print(auc_rf)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.2f})'.format(auc_rf))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('RF ROC Curve.png', bbox_inches='tight')\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(stp_test_ey, y_pred_prob_rf)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(stp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "fig = plt.figure(2)\n",
    "pyplot.plot(rf_recall, rf_precision, marker='.', label='RF')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "pyplot.title('Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#fig.savefig('RF Precision-Recall Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curve together\n",
    "plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.2f})'.format(auc_lr))\n",
    "plt.plot(fpr_dnn, tpr_dnn, label='DNN (area = {:.2f})'.format(auc_dnn))\n",
    "plt.plot(fpr_dt, tpr_dt, label='DT (area = {:.2f})'.format(auc_dt))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.2f})'.format(auc_rf)) \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all Precision-Recall curve all together\n",
    "plt.figure(1)\n",
    "#plt.xlim(0, 1)\n",
    "#plt.ylim(0, 1)\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lr_precision, lr_recall, label='LR')\n",
    "plt.plot(dnn_precision, dnn_recall, label='DNN')\n",
    "plt.plot(dt_precision, dt_recall, label='DT')\n",
    "plt.plot(rf_precision, rf_recall, label='RF') \n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Predict & Evaluate Model using Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# train the model\n",
    "sqp_rf = RandomForestClassifier()\n",
    "sqp_rf.fit(sqp_train_SX, sqp_train_y)\n",
    "# predict and evaluate performance\n",
    "sqp_rf_predictions = sqp_rf.predict(sqp_test_SX)\n",
    "meu.display_model_performance_metrics(true_labels=sqp_test_y, predicted_labels=sqp_rf_predictions, \n",
    "                                      classes=['stroke', 'control'])#classes=sqp_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Model ROC Curve\n",
    "meu.plot_model_roc_curve(sqp_rf, sqp_test_SX, sqp_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Grid Search & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Grid Search & Cross Validation\n",
    "print(sqp_rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameter values\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "                'n_estimators': [100, 200, 300, 500], \n",
    "                'max_features': ['auto', None, 'log2']    \n",
    "              }\n",
    "\n",
    "sqp_clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5,\n",
    "                       scoring='accuracy')\n",
    "sqp_clf.fit(sqp_train_SX, sqp_train_y)\n",
    "print(sqp_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View grid search results\n",
    "results = sqp_clf.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Random Forest Model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Predict & Evaluate Random Forest Model with tuned hyperparameters\n",
    "sqp_rft = RandomForestClassifier(n_estimators=200, max_features='auto', random_state=42)\n",
    "sqp_rft.fit(sqp_train_SX, sqp_train_y)\n",
    "\n",
    "sqp_rft_predictions = sqp_rft.predict(sqp_test_SX)\n",
    "meu.display_model_performance_metrics(true_labels=sqp_test_y, predicted_labels=sqp_rft_predictions, \n",
    "                                      classes=['stroke', 'control'])#classes=sqp_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/\n",
    "# Train, Predict & Evaluate Random Forest Model with tuned hyperparameters\n",
    "from sklearn.metrics import roc_curve\n",
    "stp_rft = RandomForestClassifier(n_estimators=200, max_features='auto', random_state=42)\n",
    "stp_rft.fit(sqp_train_SX, sqp_train_y)\n",
    "\n",
    "#wqp_rft_predictions = wqp_rft.predict(wqp_test_SX)\n",
    "y_pred_rft = stp_rft.predict(stp_test_SX)\n",
    "#print(y_pred_rft)\n",
    "y_pred_prob_rft = stp_rft.predict_proba(stp_test_SX)[:,1]\n",
    "\n",
    "fpr_rft, tpr_rft, thresholds_rft = roc_curve(stp_test_ey, y_pred_prob_rft)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "from sklearn.metrics import auc\n",
    "auc_rft = auc(fpr_rft, tpr_rft)\n",
    "#print(auc_rft)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rft, tpr_rft, label='RFT (area = {:.2f})'.format(auc_rft))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('RFT ROC Curve.png', bbox_inches='tight')\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "rft_precision, rft_recall, _ = precision_recall_curve(stp_test_ey, y_pred_prob_rft)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(wtp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "fig = plt.figure(2)\n",
    "pyplot.plot(rft_recall, rft_precision, marker='.', label='RFT')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "pyplot.title('Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "#fig.savefig('RFT Precision-Recall Curve.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis of Model Feature importances\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "# leveraging skater for feature importances\n",
    "interpreter = Interpretation(sqp_test_SX, feature_names=sqp_feature_names)\n",
    "sqp_im_model = InMemoryModel(sqp_rf.predict_proba, examples=sqp_train_SX, target_names=sqp_rf.classes_)\n",
    "# retrieving feature importances from the scikit-learn estimator\n",
    "sqp_rf_feature_importances = sqp_rf.feature_importances_\n",
    "sqp_rf_feature_names, sqp_rf_feature_scores = zip(*sorted(zip(sqp_feature_names, sqp_rf_feature_importances), \n",
    "                                                          key=lambda x: x[1]))\n",
    "# plot the feature importance plots\n",
    "#fig = plt.figure(1)\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "t = f.suptitle('Feature Importances for Random Forest', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.6)\n",
    "y_position = list(range(len(sqp_rf_feature_names)))\n",
    "ax1.barh(y_position, sqp_rf_feature_scores, height=0.6, align='center', tick_label=sqp_rf_feature_names)\n",
    "ax1.set_title(\"Scikit-Learn\")\n",
    "ax1.set_xlabel('Relative Importance Score')\n",
    "ax1.set_ylabel('Feature')\n",
    "plots = interpreter.feature_importance.plot_feature_importance(sqp_im_model, ascending=False, ax=ax2)\n",
    "ax2.set_title(\"Skater\")\n",
    "ax2.set_xlabel('Relative Importance Score')\n",
    "ax2.set_ylabel('Feature')\n",
    "#fig.savefig('RF Relative Importance Score.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqp_im_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Model ROC Curve\n",
    "meu.plot_model_roc_curve(sqp_rf, sqp_test_SX, sqp_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model decision surface\n",
    "feature_indices = [i for i, feature in enumerate(sqp_feature_names) \n",
    "                       if feature in ['Variability Y', 'Abs error XY']]\n",
    "meu.plot_model_decision_surface(clf=sqp_rf, train_features=sqp_train_SX[:, feature_indices], \n",
    "                      train_labels=sqp_train_y, plot_step=0.01, cmap=plt.cm.RdYlBu,\n",
    "                      markers=[',', 'd'], alphas=[0.6, 0.56], colors=['r', 'b'])\n",
    "                      #markers=[',', 'd', '+'], alphas=[1.0, 0.8, 0.5], colors=['r', 'b', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Model Predictions\n",
    "\n",
    "from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "exp = LimeTabularExplainer(sqp_train_SX, feature_names=sqp_feature_names, \n",
    "                           discretize_continuous=True, \n",
    "                           class_names=sqp_rf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.explain_instance(sqp_test_SX[10], sqp_rf.predict_proba, top_labels=1).show_in_notebook()\n",
    "#fig = plt.figure(1)\n",
    "#fig.savefig('RF Prediction Prob1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.explain_instance(sqp_test_SX[4], sqp_rf.predict_proba, top_labels=1).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.explain_instance(sqp_test_SX[1], sqp_rf.predict_proba, top_labels=1).show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing partial dependencies\n",
    "axes_list = interpreter.partial_dependence.plot_partial_dependence(['Abs error XY'], sqp_im_model, \n",
    "                                                                   grid_resolution=100, \n",
    "                                                                   with_variance=True,\n",
    "                                                                   figsize = (6, 4))\n",
    "axs = axes_list[0][3:]\n",
    "[ax.set_ylim(0, 1) for ax in axs];\n",
    "#fig.savefig('PPD Abs XY.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpreter.partial_dependence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_list = interpreter.partial_dependence.plot_partial_dependence([('Abs error XY', 'Variability Y')], sqp_im_model, n_samples=1000, figsize=(12, 5),\n",
    "                                                                    grid_resolution=100)\n",
    "axs = plots_list[0][3:]\n",
    "[ax.set_zlim(0, 1) for ax in axs];\n",
    "#fig.savefig('PPD Abs and Variability.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_list = interpreter.partial_dependence.plot_partial_dependence([('Variability XY', 'Variability Y')], sqp_im_model, n_samples=1000, figsize=(12, 5),\n",
    "                                                                    grid_resolution=100)\n",
    "axs = plots_list[0][3:]\n",
    "[ax.set_zlim(0, 1) for ax in axs];\n",
    "#fig.savefig('PPD Abs and Variability.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments for the above figure:\n",
    "We run a deeper model interpretation here over all the data samples, trying to see interactions between Abs error XY and Variability XY and also their effect on the probability of the model predicting if the person have stroke, with the help of a two-way partial dependence plot.\n",
    "Basically having a better Abs error XY and more Variability XY leads to you identify more stroke participants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Build default SVM Model\n",
    "def_svc = SVC(random_state=42)\n",
    "def_svc.fit(stp_train_SX, stp_train_y)\n",
    "\n",
    "# predict and evalute model performance\n",
    "stp_svm_predictions = def_svc.predict(stp_test_SX)\n",
    "#print(wtp_lr_predictions)\n",
    "meu.display_model_performance_metrics(true_labels=stp_test_y, predicted_labels=stp_svm_predictions, \n",
    "                                      classes=['stroke', 'control'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "stp_svm = SVC(random_state=42, probability=True)\n",
    "stp_svm.fit(stp_train_SX, stp_train_y)\n",
    "\n",
    "y_pred_svm = stp_svm.predict(stp_test_SX)\n",
    "y_pred_prob_svm = stp_svm.predict_proba(stp_test_SX)[:,1]\n",
    "\n",
    "fpr_svm, tpr_svm, thresholds_svm = roc_curve(stp_test_ey, y_pred_prob_svm)\n",
    "\n",
    "# AUC value can also be calculated like this.\n",
    "auc_svm = auc(fpr_svm, tpr_svm)\n",
    "#print(auc_lr)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (area = {:.2f})'.format(auc_svm))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('SVM ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# calculate precision and recall for each threshold\n",
    "svm_precision, svm_recall, _ = precision_recall_curve(stp_test_ey, y_pred_prob_svm)\n",
    "#print(lr_precision)\n",
    "# calculate scores\n",
    "#lr_f1, lr_auc = f1_score(wtp_test_ey, y_pred_prob_lr), auc(lr_recall, lr_precision)\n",
    "plt.figure(2)\n",
    "pyplot.plot(svm_recall, svm_precision, marker='.', label='svm')\n",
    "# summarize scores\n",
    "#print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# axis labels\n",
    "pyplot.title('SVM Precision-Recall Curve')\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretation\n",
    "# View Feature importances\n",
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "\n",
    "stp_interpreter = Interpretation(stp_test_SX, feature_names=stp_features.columns)\n",
    "stp_im_model = InMemoryModel(stp_svm.predict_proba, examples=stp_train_SX, target_names=stp_svm.classes_)\n",
    "plots = stp_interpreter.feature_importance.plot_feature_importance(stp_im_model, ascending=False)\n",
    "plt.xlabel('Relative Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for SVM')\n",
    "fig = plt.figure(1)\n",
    "#fig.savefig('LR Feature Importance.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing variable importance across all models\n",
    "models = {'lr':LogisticRegression(),\n",
    "          'dt':DecisionTreeClassifier(max_depth=4),\n",
    "          'rf':RandomForestClassifier(),\n",
    "          'rft':RandomForestClassifier(n_estimators=200, max_features='auto', random_state=42), \n",
    "          'svm':SVC(random_state=42, probability=True)}\n",
    "\n",
    "#lr.fit(stp_train_SX, stp_train_y)\n",
    "for model_key in models:\n",
    "    model = models[model_key]\n",
    "    model.fit(stp_train_SX, stp_train_y)\n",
    "    preds = model.predict(stp_test_SX)\n",
    "\n",
    "f, axes = plt.subplots(3,2, figsize = (16,16))\n",
    "\n",
    "ax_dict = {\n",
    "    'lr':axes[0][0],\n",
    "    'dt':axes[0][1],\n",
    "    'rf':axes[1][0],\n",
    "    'rft':axes[1][1],\n",
    "    'svm':axes[2][0]\n",
    "}\n",
    "\n",
    "interpreter = Interpretation(stp_test_SX, feature_names=stp_features.columns)#(X_test, feature_names=data.feature_names)\n",
    "\n",
    "for model_key in models:\n",
    "    pyint_model = InMemoryModel(models[model_key].predict_proba, examples=stp_train_SX)\n",
    "    ax = ax_dict[model_key]\n",
    "    interpreter.feature_importance.plot_feature_importance(pyint_model, ax=ax, ascending = False)\n",
    "    ax.set_title(model_key)\n",
    "    #ax1.set_title(\"Scikit-Learn\")\n",
    "    ax.set_xlabel('Relative Importance Score')\n",
    "    ax.set_ylabel('Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "stp_interpreter = Interpretation(stp_test_SX, feature_names=stp_features.columns)\n",
    "stp_im_model = InMemoryModel(stp_lr.predict_proba, examples=stp_train_SX, target_names=stp_lr.classes_)\n",
    "plots = stp_interpreter.feature_importance.plot_feature_importance(stp_im_model, ascending=False)\n",
    "\n",
    "# View Feature Importances from Decision Tree Model\n",
    "sqp_dt_feature_importances = sqp_dt.feature_importances_\n",
    "sqp_dt_feature_names, sqp_dt_feature_scores = zip(*sorted(zip(sqp_feature_names, sqp_dt_feature_importances), \n",
    "                                                          key=lambda x: x[1]))\n",
    "\n",
    "#RF\n",
    "interpreter = Interpretation(sqp_test_SX, feature_names=sqp_feature_names)\n",
    "sqp_im_model = InMemoryModel(sqp_rf.predict_proba, examples=sqp_train_SX, target_names=sqp_rf.classes_)\n",
    "# retrieving feature importances from the scikit-learn estimator\n",
    "sqp_rf_feature_importances = sqp_rf.feature_importances_\n",
    "sqp_rf_feature_names, sqp_rf_feature_scores = zip(*sorted(zip(sqp_feature_names, sqp_rf_feature_importances), \n",
    "                                                          key=lambda x: x[1]))\n",
    "#SVM\n",
    "stp_interpreter = Interpretation(stp_test_SX, feature_names=stp_features.columns)\n",
    "stp_im_model = InMemoryModel(stp_svm.predict_proba, examples=stp_train_SX, target_names=stp_svm.classes_)\n",
    "plots = stp_interpreter.feature_importance.plot_feature_importance(stp_im_model, ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve for all networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curve together\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.2f})'.format(auc_lr))\n",
    "plt.plot(fpr_dt, tpr_dt, label='DT (area = {:.2f})'.format(auc_dt))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.2f})'.format(auc_rf))\n",
    "plt.plot(fpr_rft, tpr_rft, label='RFT (area = {:.2f})'.format(auc_rft))\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (area = {:.2f})'.format(auc_rft))\n",
    "plt.plot(fpr_dnn, tpr_dnn, label='DNN (area = {:.2f})'.format(auc_dnn))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('All ROC Curves.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all ROC curve together\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.3f})'.format(auc_lr))\n",
    "plt.plot(fpr_dnn, tpr_dnn, label='DNN (area = {:.3f})'.format(auc_dnn))\n",
    "plt.plot(fpr_dt, tpr_dt, label='DT (area = {:.3f})'.format(auc_dt))\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.plot(fpr_rft, tpr_rft, label='RFT (area = {:.3f})'.format(auc_rft))\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM (area = {:.3f})'.format(auc_rft))\n",
    "plt.xlabel('False Positive Rate/Specificity')\n",
    "plt.ylabel('True Positive Rate/Sensitivity')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "#fig.savefig('All ROC Curves.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all Precision-Recall curve all together\n",
    "fig = plt.figure(1)\n",
    "#plt.xlim(0, 1)\n",
    "#plt.ylim(0, 1)\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lr_precision, lr_recall, label='LR')\n",
    "plt.plot(dt_precision, dt_recall, label='DT')\n",
    "plt.plot(rf_precision, rf_recall, label='RF')\n",
    "plt.plot(rft_precision, rft_recall, label='RFT')\n",
    "plt.plot(svm_precision, svm_recall, label='SVM') \n",
    "plt.plot(dnn_precision, dnn_recall, label='DNN')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#fig.savefig('All Precision-Recall Curves.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all Precision-Recall curve all together\n",
    "plt.figure(1)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(lr_precision, lr_recall, label='LR')\n",
    "plt.plot(dt_precision, dt_recall, label='DT')\n",
    "plt.plot(rf_precision, rf_recall, label='RF')\n",
    "plt.plot(rft_precision, rft_recall, label='RFT')\n",
    "plt.plot(svm_precision, svm_recall, label='SVM')\n",
    "plt.plot(dnn_precision, dnn_recall, label='DNN')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
